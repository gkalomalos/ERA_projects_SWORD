{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scenario\n",
    "from copy import deepcopy\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from constants import DATA_ENTITIES_DIR, DATA_HAZARDS_DIR, DATA_TEMP_DIR, REQUIREMENTS_DIR\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from climada.engine import CostBenefit, Impact, ImpactCalc\n",
    "from climada.engine.cost_benefit import risk_aai_agg, risk_rp_100, risk_rp_250\n",
    "from climada.entity import DiscRates, Entity, Exposures, LitPop\n",
    "from climada.entity.impact_funcs import ImpactFunc, ImpactFuncSet, ImpfTropCyclone\n",
    "from climada.entity.impact_funcs.storm_europe import ImpfStormEurope\n",
    "from climada.entity.impact_funcs.trop_cyclone import ImpfSetTropCyclone\n",
    "from climada.entity.measures import Measure, MeasureSet\n",
    "from climada.hazard import Hazard\n",
    "from climada.util.api_client import Client\n",
    "\n",
    "from costben.costben_handler import CostBenefitHandler\n",
    "from entity.entity_handler import EntityHandler\n",
    "from exposure.exposure_handler import ExposureHandler\n",
    "from hazard.hazard_handler import HazardHandler\n",
    "from impact.impact_handler import ImpactHandler\n",
    "\n",
    "from base_handler import BaseHandler\n",
    "from logger_config import LoggerConfig\n",
    "\n",
    "logger = LoggerConfig(logger_types=[\"file\"])\n",
    "\n",
    "base_handler = BaseHandler()\n",
    "costben_handler = CostBenefitHandler()\n",
    "entity_handler = EntityHandler()\n",
    "exposure_handler = ExposureHandler()\n",
    "hazard_handler = HazardHandler()\n",
    "impact_handler = ImpactHandler()\n",
    "\n",
    "\n",
    "# Available Exposure data types in CLIMADA API for Egypt/Thailand: ['litpop']\n",
    "# Available Hazard data types in CLIMADA API for Egypt/Thailand: ['river_flood', 'wildfire', 'earthquake', flood, 'tropical_cyclone']\n",
    "# Available climate scenarios for hazard type river_flood/tropical_c in country Egypt/Thailand: ['rcp26', 'historical', 'rcp60', 'rcp85']\n",
    "# Available time horizons for hazard type river_flood in country Egypt: ['2030_2050', '1980_2000', '2070_2090', '2010_2030', '2050_2070']\n",
    "\n",
    "country_name = \"Egypt\"\n",
    "exposure_type = \"litpop\"  # Available exposure types for Egypt/Thailand: ['litpop']\n",
    "hazard_type = \"river_flood\"  # Available hazard types for Egypt/Thailand: ['river_flood', 'wildfire', 'earthquake', 'flood', 'tropical_cyclone']\n",
    "scenario = \"rcp26\"  # Available scenarios for Egypt/Thailand: ['rcp26', 'historical', 'rcp60', 'rcp85']\n",
    "time_horizon = \"2030_2050\"  # Available time horizons for Egypt/Thailand: ['2030_2050', '1980_2000', '2070_2090', '2010_2030', '2050_2070']\n",
    "annual_growth = 1.01\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# growth_rates = {\n",
    "#     \"Egypt\": {\n",
    "#         \"crops\": 0.04,\n",
    "#         \"livestock\": 0.04,\n",
    "#         \"power_plants\": 0.04,\n",
    "#         \"hotels\": 0.04,\n",
    "#         \"hospitalised_people\": 0.0129,\n",
    "#         \"students\": 0.0129,\n",
    "#         \"diarrhea_patients\": 0.0129,\n",
    "#         \"roads\": 0.0129,\n",
    "#     },\n",
    "#     \"Thailand\": {\n",
    "#         \"tree_crops\": 0.0294,\n",
    "#         \"grass_crops\": 0.0294,\n",
    "#         \"wet_markets\": 0.0294,\n",
    "#         \"grass_crops_farmers\": -0.0022,\n",
    "#         \"tree_crops_farmers\": -0.0022,\n",
    "#         \"buddhist_monks\": -0.0022,\n",
    "#         \"diarrhea_patients\": -0.0022,\n",
    "#         \"students\": -0.0022,\n",
    "#         \"roads\": -0.0022,\n",
    "#     },\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data files\n",
    "import h5py\n",
    "import rasterio\n",
    "\n",
    "def read_mat():\n",
    "    with h5py.File(DATA_HAZARDS_DIR / \"hazard_HW_EGY_rcp45.mat\", 'r') as f:\n",
    "        # Print all items in the root\n",
    "        print(\"Items in the root:\", list(f.keys()))\n",
    "\n",
    "        # Access the 'hazard' group/dataset\n",
    "        hazard = f['hazard']\n",
    "        print(\"Items in 'hazard':\", list(hazard.keys()))\n",
    "\n",
    "        # If 'hazard' contains further groups or datasets, access them\n",
    "        for item in hazard:\n",
    "            print(f\"Exploring {item}:\")\n",
    "            data = hazard[item]\n",
    "            \n",
    "            # Check if this is a dataset or a group\n",
    "            if isinstance(data, h5py.Dataset):\n",
    "                print(f\"Dataset {item} found with shape {data.shape} and data type {data.dtype}\")\n",
    "            else:\n",
    "                print(f\"Group {item} contains: {list(data.keys())}\")\n",
    "\n",
    "            # If the item is stored by reference (common in MATLAB structures)\n",
    "            if data.dtype == 'O':  # Object references\n",
    "                # This will go through each reference and try to resolve it\n",
    "                for ref in data:\n",
    "                    referenced_object = f[data[ref][0]]  # Access by reference\n",
    "                    print(f\"Referenced object for {ref}: {referenced_object.shape}\")\n",
    "\n",
    "                    # Optionally, you can load the data into an array or similar\n",
    "                    # print(np.array(referenced_object))\n",
    "\n",
    "def read_tif():\n",
    "    with rasterio.open(DATA_HAZARDS_DIR / \"hazard_HW_EGY_rcp45.tif\") as src:\n",
    "        print(f'Number of bands: {src.count}')\n",
    "        # Loop through each band\n",
    "        # for i in range(1, src.count + 1):\n",
    "        #     band = src.read(i)\n",
    "        #     print(band)\n",
    "        #     meta = src.tags(i)\n",
    "        #     print(f'  Metadata for Band {i}: {meta}')\n",
    "\n",
    "\n",
    "\n",
    "def explore_h5_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads and explores an HDF5 (.h5) file, printing its structure and basic data info.\n",
    "\n",
    "    :param file_path: The path to the HDF5 file.\n",
    "    :type file_path: str or Path\n",
    "    \"\"\"\n",
    "    def explore_group(group, level=0):\n",
    "        \"\"\"\n",
    "        Recursively explores a group in the HDF5 file, printing dataset information.\n",
    "\n",
    "        :param group: The HDF5 group to explore.\n",
    "        :type group: h5py.Group\n",
    "        :param level: The current depth level in the group hierarchy (for indentation).\n",
    "        :type level: int\n",
    "        \"\"\"\n",
    "        indent = \"  \" * level  # To visualize hierarchy\n",
    "\n",
    "        for key in group.keys():\n",
    "            item = group[key]\n",
    "            if isinstance(item, h5py.Group):\n",
    "                print(f\"{indent}Group: {key}\")\n",
    "                # Recursively explore this group\n",
    "                explore_group(item, level + 1)\n",
    "            elif isinstance(item, h5py.Dataset):\n",
    "                print(f\"{indent}Dataset: {key}\")\n",
    "                print(f\"{indent}  Shape: {item.shape}\")\n",
    "                print(f\"{indent}  Data type: {item.dtype}\")\n",
    "                # Print attributes if available\n",
    "                if item.attrs:\n",
    "                    print(f\"{indent}  Attributes: {dict(item.attrs)}\")\n",
    "            else:\n",
    "                print(f\"{indent}Other: {key}\")\n",
    "\n",
    "    # Open the HDF5 file\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        print(f\"Exploring HDF5 file: {file_path}\")\n",
    "        # Explore the root group\n",
    "        explore_group(f)\n",
    "\n",
    "# Example usage\n",
    "explore_h5_file(DATA_HAZARDS_DIR / \"Thai_DR_RCP_45.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example App\n",
    "entity_present = entity_handler.get_entity_from_xlsx(\n",
    "    DATA_ENTITIES_DIR / \"entity_TODAY_EGY_HW_livestock.xlsx\"\n",
    ")\n",
    "entity_present.check()\n",
    "entity_present.exposures.ref_year = 2024\n",
    "\n",
    "entity_future = entity_handler.get_future_entity(entity_present, 2050, 0.04)\n",
    "entity_future.check()\n",
    "\n",
    "hazard_present = hazard_handler.get_hazard(\n",
    "    hazard_type=\"heatwaves\", filepath=DATA_HAZARDS_DIR / \"hazard_HW_EGY_historical.h5\"\n",
    ")\n",
    "hazard_present.haz_type = \"HW\"\n",
    "hazard_present.check()\n",
    "\n",
    "hazard_future = hazard_handler.get_hazard(\n",
    "    hazard_type=\"heatwaves\", filepath=DATA_HAZARDS_DIR / \"hazard_HW_EGY_rcp45.h5\"\n",
    ")\n",
    "hazard_future.haz_type = \"HW\"\n",
    "hazard_future.check()\n",
    "\n",
    "hazard_present.units = hazard_handler.get_hazard_intensity_units_from_entity(entity_present)\n",
    "hazard_future.units = hazard_handler.get_hazard_intensity_units_from_entity(entity_future)\n",
    "\n",
    "hazard_present.centroids.set_geometry_points()\n",
    "hazard_future.centroids.set_geometry_points()\n",
    "\n",
    "hazard_present.intensity_thres = hazard_handler.get_hazard_intensity_thres(\"HW\")\n",
    "hazard_future.intensity_thres = hazard_handler.get_hazard_intensity_thres(\"HW\")\n",
    "\n",
    "# Calculate impact\n",
    "impact_present = ImpactCalc(\n",
    "    entity_present.exposures, entity_present.impact_funcs, hazard_present\n",
    ").impact(save_mat=True)\n",
    "impact_future = ImpactCalc(\n",
    "    entity_future.exposures, entity_future.impact_funcs, hazard_future\n",
    ").impact(save_mat=True)\n",
    "\n",
    "\n",
    "costben = costben_handler.calculate_cost_benefit(\n",
    "    hazard_present, entity_present, hazard_future, entity_future, 2050\n",
    ")\n",
    "# costben_handler.plot_cost_benefit(costben)\n",
    "# costben_handler.plot_waterfall(\n",
    "#     costben, hazard_present, entity_present, hazard_future, entity_future\n",
    "# )\n",
    "# ax = costben.plot_waterfall(hazard_present, entity_present, hazard_future, entity_future)\n",
    "costben.plot_cost_benefit()\n",
    "\n",
    "# exposure_handler.generate_exposure_geojson(entity_future.exposures, \"Thailand\")\n",
    "# hazard_handler.generate_hazard_geojson(hazard_future, \"Thailand\")\n",
    "# impact_handler.generate_impact_geojson(impact_future, \"Thailand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example CLIMADA\n",
    "entity_present = Entity.from_excel(\n",
    "    DATA_ENTITIES_DIR / \"entity_TODAY_THA_D_tree_crops.xlsx\"\n",
    ")\n",
    "# Set exposure ref year\n",
    "entity_present.exposures.ref_year = 2024\n",
    "entity_present.check()\n",
    "\n",
    "# Set future Entity\n",
    "entity_future = deepcopy(entity_present)\n",
    "entity_future.exposures.ref_year = 2050\n",
    "growth = 0.029\n",
    "entity_future.exposures.gdf[\"value\"] = entity_future.exposures.gdf.value.values * (1 + growth) ** (\n",
    "    entity_future.exposures.ref_year - entity_present.exposures.ref_year\n",
    ")\n",
    "entity_future.check()\n",
    "\n",
    "# Set present Hazard\n",
    "hazard_present = Hazard.from_hdf5(\n",
    "    DATA_HAZARDS_DIR / \"hazard_D_THA_historical.h5\",\n",
    ")\n",
    "hazard_present.units = \"m\"\n",
    "hazard_present.centroids.set_geometry_points()\n",
    "hazard_present.intensity_thres = -4\n",
    "hazard_present.check()\n",
    "\n",
    "# Set future hazard\n",
    "hazard_future = Hazard.from_hdf5(\n",
    "    DATA_HAZARDS_DIR / \"hazard_HW_EGY_rcp45.h5\",\n",
    "    attrs={\n",
    "        \"frequency\": np.array([0.5, 0.2, 0.1, 0.04]),\n",
    "        \"event_id\": np.array([1, 2, 3, 4]),\n",
    "        \"units\": \"number of days\",\n",
    "    },\n",
    "    haz_type=\"HW\",\n",
    "    band=[1, 2, 3, 4],\n",
    ")\n",
    "hazard_future.units = \"number of days\"\n",
    "hazard_future.centroids.set_geometry_points()\n",
    "hazard_future.intensity_thres = 0\n",
    "hazard_future.check()\n",
    "\n",
    "# Calculate impact\n",
    "impact_present = ImpactCalc(\n",
    "    entity_present.exposures, entity_present.impact_funcs, hazard_present\n",
    ").impact()\n",
    "impact_future = ImpactCalc(\n",
    "    entity_future.exposures, entity_future.impact_funcs, hazard_future\n",
    ").impact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List DataTypeInfos\n",
    "data_type_infos = client.list_data_type_infos()\n",
    "exposure_data_types = [\n",
    "    data_type_info.data_type\n",
    "    for data_type_info in data_type_infos\n",
    "    if data_type_info.data_type_group == \"exposures\"\n",
    "]\n",
    "hazard_data_types = [\n",
    "    data_type_info.data_type\n",
    "    for data_type_info in data_type_infos\n",
    "    if data_type_info.data_type_group == \"hazard\"\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"Available Exposure data types in CLIMADA API for all countries:\\n{exposure_data_types}\")\n",
    "print(\n",
    "    f\"Available Hazard data types in CLIMADA API for all countries:\\n{hazard_data_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available country Exposures and Hazard data types\n",
    "dataset_infos = client.list_dataset_infos(\n",
    "    properties={\n",
    "        \"country_name\": country_name,\n",
    "    }\n",
    ")\n",
    "\n",
    "exposure_data_types = list(\n",
    "    set(\n",
    "        [\n",
    "            dataset_info.data_type.data_type\n",
    "            for dataset_info in dataset_infos\n",
    "            if dataset_info.data_type.data_type_group == \"exposures\"\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "hazard_data_types = list(\n",
    "    set(\n",
    "        [\n",
    "            dataset_info.data_type.data_type\n",
    "            for dataset_info in dataset_infos\n",
    "            if dataset_info.data_type.data_type_group == \"hazard\"\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Available Exposure data types in CLIMADA API for {country_name}: {exposure_data_types}\")\n",
    "print(\n",
    "    f\"Available Hazard data types in CLIMADA API for {country_name}: {hazard_data_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available climate scenarios and time horizons for specific hazard type in countries Thailand and Egypt\n",
    "if hazard_type == \"river_flood\" or \"wildfire\":\n",
    "    available_scenarios = list(\n",
    "        set(\n",
    "            [\n",
    "                dataset_info.properties[\"climate_scenario\"]\n",
    "                for dataset_info in dataset_infos\n",
    "                if dataset_info.data_type.data_type == hazard_type\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    available_time_horizons = list(\n",
    "        set(\n",
    "            [\n",
    "                dataset_info.properties[\"year_range\"]\n",
    "                for dataset_info in dataset_infos\n",
    "                if dataset_info.data_type.data_type == hazard_type\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "if hazard_type == \"tropical_cyclone\":\n",
    "    available_scenarios = list(\n",
    "        set(\n",
    "            [\n",
    "                dataset_info.properties[\"climate_scenario\"]\n",
    "                for dataset_info in dataset_infos\n",
    "                if dataset_info.data_type.data_type == hazard_type\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    available_time_horizons = list(\n",
    "        set(\n",
    "            [\n",
    "                dataset_info.properties.get(\"ref_year\")\n",
    "                for dataset_info in dataset_infos\n",
    "                if dataset_info.data_type.data_type == hazard_type\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "if hazard_type == \"earthquake\":\n",
    "    available_scenarios = []\n",
    "    available_time_horizons = []\n",
    "if hazard_type == \"flood\":\n",
    "    available_scenarios = []\n",
    "    available_time_horizons = list(\n",
    "        set(\n",
    "            [\n",
    "                dataset_info.properties[\"year_range\"]\n",
    "                for dataset_info in dataset_infos\n",
    "                if dataset_info.data_type.data_type == hazard_type\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\n",
    "    f\"Available climate scenarios for hazard type {hazard_type} in country {country_name}: {available_scenarios}\"\n",
    ")\n",
    "print(\n",
    "    f\"Available time horizons for hazard type {hazard_type} in country {country_name}: {available_time_horizons}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available country Hazard DatasetInfos\n",
    "hazard_dataset_infos = client.list_dataset_infos(\n",
    "    properties={\n",
    "        \"data_type\": \"river_flood\",\n",
    "        \"country_name\": \"Thailand\",\n",
    "        \"climate_scenario\": \"rcp26\",\n",
    "        \"year_range\": \"2030_2050\",\n",
    "    }\n",
    ")\n",
    "hazard_dataset_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "class DataFrameSQLite:\n",
    "    def __init__(self, db_path):\n",
    "        self.db_path = db_path\n",
    "\n",
    "    def save_dataframe(self, df, table_name):\n",
    "        \"\"\"\n",
    "        Save a DataFrame to the SQLite database.\n",
    "        \n",
    "        :param df: DataFrame to save.\n",
    "        :param table_name: Name of the table to save the DataFrame to.\n",
    "        \"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "            print(f\"DataFrame saved to {table_name} table.\")\n",
    "\n",
    "    def read_dataframe(self, table_name):\n",
    "        \"\"\"\n",
    "        Read a DataFrame from the SQLite database.\n",
    "        \n",
    "        :param table_name: Name of the table to read the DataFrame from.\n",
    "        :return: DataFrame read from the database.\n",
    "        \"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            df = pd.read_sql(f\"SELECT * FROM {table_name}\", conn)\n",
    "        return df\n",
    "\n",
    "# Example usage\n",
    "db_path = 'my_data.db'  # Path to your SQLite database file\n",
    "df_sqlite = DataFrameSQLite(db_path)\n",
    "\n",
    "# Assuming you have a DataFrame `df` to save\n",
    "df_sqlite.save_dataframe(exp_gdf, 'exposure')\n",
    "\n",
    "# To read the saved DataFrame from the database\n",
    "df_read = df_sqlite.read_dataframe('exposure')\n",
    "print(df_read)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "class ExcelToSQLite:\n",
    "    def __init__(self, db_path):\n",
    "        self.db_path = db_path\n",
    "\n",
    "    def read_excel_to_df(self, excel_path, sheet_name):\n",
    "        \"\"\"Reads a specified sheet from an Excel file into a DataFrame.\"\"\"\n",
    "        return pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "\n",
    "    def adjust_df_columns(self, df, columns_to_keep=None):\n",
    "        \"\"\"Adjusts DataFrame columns based on the provided list. If None, keeps all columns.\"\"\"\n",
    "        if columns_to_keep is not None:\n",
    "            df = df[columns_to_keep]\n",
    "        return df\n",
    "\n",
    "    def save_df_to_sqlite(self, df, table_name):\n",
    "        \"\"\"Saves a DataFrame to an SQLite table, appending data if the table already exists.\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            df.to_sql(table_name, conn, if_exists=\"append\", index=False)\n",
    "\n",
    "    def read_table(self, table_name):\n",
    "        \"\"\"Reads a table from SQLite database into a DataFrame.\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            return pd.read_sql(f\"SELECT * FROM {table_name}\", conn)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "db_path = \"climadera.db\"  # Path to your SQLite database file\n",
    "excel_path = DATA_ENTITIES_DIR / \"3_entity_TODAY_THAI_D_USD_1_modified.xlsx\"  # Path to your Excel file\n",
    "excel_to_sqlite = ExcelToSQLite(db_path)\n",
    "\n",
    "# Define the sheets and corresponding table names\n",
    "sheets_tables = {\n",
    "    \"assets\": \"exposures\",\n",
    "    \"impact_functions\": \"impact_functions\",\n",
    "    \"measures\": \"measures\",\n",
    "    \"discount\": \"discount_rates\",\n",
    "    \"names\": \"names\",\n",
    "}\n",
    "\n",
    "# Iterate over sheets and tables, read, adjust (if needed), and save to SQLite\n",
    "for sheet, table in sheets_tables.items():\n",
    "    df = excel_to_sqlite.read_excel_to_df(excel_path, sheet)\n",
    "\n",
    "    # Here you can define which columns to keep for each table if needed, e.g.:\n",
    "    # if table == 'exposures':\n",
    "    #     columns_to_keep = ['Column1', 'Column2']\n",
    "    #     df = excel_to_sqlite.adjust_df_columns(df, columns_to_keep)\n",
    "    # else:\n",
    "    #     df = excel_to_sqlite.adjust_df_columns(df)\n",
    "\n",
    "    excel_to_sqlite.save_df_to_sqlite(df, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "class ExcelToSQLite:\n",
    "    def __init__(self, db_path):\n",
    "        self.db_path = db_path\n",
    "\n",
    "    def read_excel_and_save(self, excel_path, tabs_columns_mapping):\n",
    "        \"\"\"\n",
    "        Read specified tabs from an Excel file and save them to SQLite database.\n",
    "\n",
    "        :param excel_path: Path to the Excel file.\n",
    "        :param tabs_columns_mapping: Dict mapping tab names to their columns adjustments.\n",
    "        \"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            for tab, columns in tabs_columns_mapping.items():\n",
    "                df = pd.read_excel(excel_path, sheet_name=tab)\n",
    "\n",
    "                # Adjust columns if specified\n",
    "                if columns:\n",
    "                    df = df[columns]\n",
    "\n",
    "                df.to_sql(tab, conn, if_exists=\"append\", index=False)\n",
    "                print(f\"Data from {tab} tab saved to {tab} table.\")\n",
    "\n",
    "    def read_table(self, table_name):\n",
    "        \"\"\"\n",
    "        Read data from a specified table in the SQLite database.\n",
    "\n",
    "        :param table_name: Name of the table to read data from.\n",
    "        :return: DataFrame with the table data.\n",
    "        \"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            return pd.read_sql(f\"SELECT * FROM {table_name}\", conn)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "db_path = \"climadera.db\"  # Path to your SQLite database file\n",
    "excel_path = DATA_ENTITIES_DIR / \"3_entity_TODAY_THAI_D_USD_1_modified.xlsx\"  # Path to your Excel file\n",
    "tabs_columns_mapping = {\n",
    "    \"assets\": None,  # Specify columns as a list if you want to adjust them, or None to include all\n",
    "    \"impact_functions\": None,\n",
    "    \"measures\": None,\n",
    "    \"discount\": None,\n",
    "    \"names\": None,\n",
    "}\n",
    "\n",
    "excel_to_sqlite = ExcelToSQLite(db_path)\n",
    "excel_to_sqlite.read_excel_and_save(excel_path, tabs_columns_mapping)\n",
    "\n",
    "# Reading data from one of the tables\n",
    "df_assets = excel_to_sqlite.read_table(\"assets\")\n",
    "print(df_assets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
